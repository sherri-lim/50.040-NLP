{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50.040 Natural Language Processing (Fall 2025) Homework 2\n",
    "\n",
    "**Due 26 October 2025, 23:59 PM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### STUDNET ID: 1006922\n",
    " \n",
    "### Name: Lim Sherri\n",
    "\n",
    "### Students with whom you have discussed (if any):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install torch\n",
    "#!pip3 install d2l\n",
    "#pip3 install torchvision\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the core challenges in NLP is how to represent language, the nature of which lies in its sequential structure of words. To address this, various sequence modeling architectures have been proposed. The most prominent models include the early RNN (Recurrent Neural Network), LSTM (Long Short-Term Memory network), and the more recent Transformer. They differ primarily in how they processes information. RNN processes sequences step by step but struggles with long-range dependencies. LSTM introduces gating mechanisms to selectively retain information over long sequences. In contrast, Transformer relies on attention mechanisms to capture dependencies across all positions in a sequence.\n",
    "\n",
    "This homework provides a concise review of RNN and LSTM, with particular emphasis on how they model contextual information. It then shifts focus to attention mechanism, introducing its basic operations, its applications in sequence-to-sequence learning, and its multi-head variant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN and LSTM\n",
    "The primary challenge of processing a sequence $(\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_T)$ lies in capturing contextual dependencies among elements. Although conventional feedforward neural networks treat each element independently, RNN introduces a hidden state $\\mathbf{h}_t$ that serves as an internal memory, allowing the model to retain and utilize information from previous time steps.\n",
    "\n",
    "![rnn](rnn.png)\n",
    "\n",
    "For a simple RNN with hidden size $h$ and input size $d$, the update of $\\mathbf{h}_t$ is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{h}_t = \\tanh(\\mathbf{x}_t \\mathbf{W}_x + \\mathbf{h}_{t-1} \\mathbf{W}_h + \\mathbf{b})\n",
    "\\end{equation}\n",
    "\n",
    "Here, $\\mathbf{x}_t \\in \\mathcal{R}^{d}$ represents the input at the current time step, which could be a word embedding or a feature vector. The term $\\mathbf{h}_{t-1} \\in \\mathcal{R}^{h}$ is the hidden state from the previous step. The matrices $\\mathbf{W}_x \\in \\mathcal{R}^{d \\times h}$ and $\\mathbf{W}_h \\in \\mathcal{R}^{h \\times h}$ transform the input and the previous hidden state into the hidden space, while $\\mathbf{b} \\in \\mathcal{R}^{h}$ is a bias term. The $\\tanh$ introduces nonlinearity into the model, squashing the combined input to values between -1 and 1.\n",
    "\n",
    "The hidden state of RNN naturally propagates information through time. However, when the sequence becomes long, RNN faces difficulties such as vanishing and exploding gradients, which make RNN hard to learn long-term dependencies.\n",
    "\n",
    "To address this, LSTM was introduced. It extends RNN by adding a cell state $\\mathbf{c}_t$ for long-term memory and a set of gates that control which information to remember, forget, or output.\n",
    "\n",
    "![lstm](lstm.png)\n",
    "\n",
    "The forward pass for an LSTM at a single time step updates both the hidden state $\\mathbf{h}_t$ and the cell state $\\mathbf{c}_t$ as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\mathbf{f}_t &= \\sigma\\big(\\mathbf{x}_t \\mathbf{W}_f + \\mathbf{h}_{t-1} \\mathbf{U}_f + \\mathbf{b}_f\\big), \\\\\n",
    "\\mathbf{i}_t &= \\sigma\\big(\\mathbf{x}_t \\mathbf{W}_i + \\mathbf{h}_{t-1} \\mathbf{U}_i + \\mathbf{b}_i\\big), \\\\\n",
    "\\mathbf{o}_t &= \\sigma\\big(\\mathbf{x}_t \\mathbf{W}_o + \\mathbf{h}_{t-1} \\mathbf{U}_o + \\mathbf{b}_o\\big), \\\\\n",
    "\\tilde{\\mathbf{c}}_t &= \\tanh\\big(\\mathbf{x}_t \\mathbf{W}_c + \\mathbf{h}_{t-1} \\mathbf{U}_c + \\mathbf{b}_c\\big), \\\\\n",
    "\\mathbf{c}_t &= \\mathbf{f}_t \\odot \\mathbf{c}_{t-1} + \\mathbf{i}_t \\odot \\tilde{\\mathbf{c}}_t, \\\\\n",
    "\\mathbf{h}_t &= \\mathbf{o}_t \\odot \\tanh(\\mathbf{c}_t).\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Here, $\\mathbf{f}_t$, $\\mathbf{i}_t$, and $\\mathbf{o}_t$ are the forget, input, and output gates, respectively. The forget gate $\\mathbf{f}_t$ determines how much of the previous cell state $\\mathbf{c}_{t-1}$ should be retained. The input gate $\\mathbf{i}_t$ controls how much of the new candidate memory $\\tilde{\\mathbf{c}}_t$ should be added to the cell state. The updated cell state $\\mathbf{c}_t$ combines retained memory and newly selected information. The output gate $\\mathbf{o}_t$ modulates the information from the cell state that will be exposed as the hidden state $\\mathbf{h}_t$. The element-wise multiplication $\\odot$ ensures selective gating, allowing the network to retain or suppress specific components of memory. The $\\mathbf{W}_f, \\mathbf{W}_i, \\mathbf{W}_o, \\mathbf{W}_c \\in \\mathcal{R}^{d \\times h}$ and $\\mathbf{U}_f, \\mathbf{U}_i, \\mathbf{U}_o, \\mathbf{U}_c \\in \\mathcal{R}^{h \\times h}$ are weight matrices of parameters conducting projections. The function $\\sigma$ represents the sigmoid activation, which outputs values between 0 and 1, controlling how much information flows through each gate.\n",
    "\n",
    "\n",
    "Compared to a vanilla RNN, LSTM gives finer control over memory. It can remember things for a long time and forget things when they are no longer relevant. This is why it performs much better at capturing long-term dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 [code] (10 points)\n",
    "\n",
    "Implement the single-step forward pass for both an RNN and an LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step(x_t, h_prev, W_x, W_h, b):\n",
    "    \"\"\"\n",
    "    Single-step forward pass for a vanilla RNN.\n",
    "    \"\"\"\n",
    "    # Shape of x_t: (d,)\n",
    "    # Shape of h_prev: (h,)\n",
    "    # Shape of W_x: (d, h)\n",
    "    # Shape of W_h: (h, h)\n",
    "    # Shape of b: (h,)\n",
    "    \n",
    "    ### YOUR CODE HERE (5 points)\n",
    "    h_t = torch.tanh(x_t @ W_x + h_prev @ W_h + b)\n",
    "    ### END OF YOUR CODE\n",
    "    return h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_step(x_t, h_prev, c_prev, W_f, U_f, b_f, W_i, U_i, b_i,\n",
    "              W_o, U_o, b_o, W_c, U_c, b_c):\n",
    "    \"\"\"\n",
    "    Single-step forward pass for an LSTM.\n",
    "    \"\"\"\n",
    "    # Shape of x_t: (d,)\n",
    "    # Shape of h_prev: (h,)\n",
    "    # Shape of c_prev: (h,)\n",
    "    # Shape of W_*, U_*, b_*: (d, h), (h, h), (h,)\n",
    "\n",
    "    ### YOUR CODE HERE (5 points)\n",
    "    f_t = torch.sigmoid(x_t @ W_f + h_prev @ U_f + b_f)\n",
    "    i_t = torch.sigmoid(x_t @ W_i + h_prev @ U_i + b_i) \n",
    "    c_tilde = torch.tanh(x_t @ W_c + h_prev @ U_c + b_c)\n",
    "    c_t = f_t * c_prev + i_t * c_tilde \n",
    "    o_t = torch.sigmoid(x_t @ W_o + h_prev @ U_o + b_o)\n",
    "    h_t = o_t * torch.tanh(c_t)\n",
    "    ### END OF YOUR CODE\n",
    "    return h_t, c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass!\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "d, h = 4, 3\n",
    "\n",
    "x_t = torch.randn(d)\n",
    "h_prev = torch.randn(h)\n",
    "c_prev = torch.randn(h)\n",
    "\n",
    "W_x, W_h, b = torch.randn(d,h), torch.randn(h,h), torch.randn(h)\n",
    "W_f, U_f, b_f = torch.randn(d,h), torch.randn(h,h), torch.randn(h)\n",
    "W_i, U_i, b_i = torch.randn(d,h), torch.randn(h,h), torch.randn(h)\n",
    "W_o, U_o, b_o = torch.randn(d,h), torch.randn(h,h), torch.randn(h)\n",
    "W_c, U_c, b_c = torch.randn(d,h), torch.randn(h,h), torch.randn(h)\n",
    "\n",
    "h_rnn = rnn_step(x_t, h_prev, W_x, W_h, b)\n",
    "d2l.check_shape(h_rnn, (h,))\n",
    "\n",
    "h_lstm, c_lstm = lstm_step(x_t, h_prev, c_prev, W_f, U_f, b_f, W_i, U_i, b_i,\n",
    "                            W_o, U_o, b_o, W_c, U_c, b_c)\n",
    "d2l.check_shape(h_lstm, (h,))\n",
    "d2l.check_shape(c_lstm, (h,))\n",
    "print(\"Pass!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 [code] (10 points)\n",
    "\n",
    "Implement the sequence-level forward functions that iterate through all time steps, updating the hidden state, and in the case of LSTM, also updating the cell state at each step. \n",
    "\n",
    "You should leverage the single-step functions implemented in Question 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(X, h0, W_x, W_h, b):\n",
    "    \"\"\"\n",
    "    Full-sequence forward pass for a vanilla RNN.\n",
    "    \"\"\"\n",
    "    # Shape of X: (T, d)\n",
    "    # Shape of h0: (h,)\n",
    "    # Shape of W_x: (d, h)\n",
    "    # Shape of W_h: (h, h)\n",
    "    # Shape of b: (h,)\n",
    "\n",
    "    T = X.shape[0]\n",
    "    H_seq = []\n",
    "    h_t = h0\n",
    "    ### YOUR CODE HERE (5 points)\n",
    "    for t in range(T):\n",
    "        h_t = rnn_step(X[t], h_t, W_x, W_h, b)\n",
    "        H_seq.append(h_t)\n",
    "    ### END OF YOUR CODE\n",
    "    H_seq = torch.stack(H_seq)\n",
    "    return H_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_forward(X, h0, c0, W_f, U_f, b_f, W_i, U_i, b_i,\n",
    "                 W_o, U_o, b_o, W_c, U_c, b_c):\n",
    "    \"\"\"\n",
    "    Full-sequence forward pass for an LSTM.\n",
    "    \"\"\"\n",
    "    # Shape of X: (T, d)\n",
    "    # Shape of h0: (h,)\n",
    "    # Shape of c0: (h,)\n",
    "    # Shape of W_*, U_*, b_*: (d, h), (h, h), (h,)\n",
    "    \n",
    "    T = X.shape[0]\n",
    "    H_seq, C_seq = [], []\n",
    "    h_t, c_t = h0, c0\n",
    "    ### YOUR CODE HERE (5 points)\n",
    "    for t in range(T):\n",
    "        h_t,c_t = lstm_step(x_t, h_t, c_t, W_f, U_f, b_f, W_i, U_i, b_i, W_o, U_o, b_o, W_c, U_c, b_c)\n",
    "        H_seq.append(h_t)\n",
    "        C_seq.append(c_t)\n",
    "    ### END OF YOUR CODE\n",
    "    H_seq = torch.stack(H_seq)\n",
    "    C_seq = torch.stack(C_seq)\n",
    "    return H_seq, C_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass!\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "T, D, H = 5, 4, 3  # sequence length, input dimension, hidden size\n",
    "X = torch.randn(T, D)\n",
    "h0 = torch.zeros(H)\n",
    "c0 = torch.zeros(H)\n",
    "\n",
    "H_rnn = rnn_forward(X, h0, W_x, W_h, b)\n",
    "d2l.check_shape(H_rnn, (5, 3))\n",
    "\n",
    "H_lstm, C_lstm = lstm_forward(X, h0, c0, W_f, U_f, b_f, W_i, U_i, b_i,\n",
    "                              W_o, U_o, b_o, W_c, U_c, b_c)\n",
    "d2l.check_shape(H_lstm, (5, 3))\n",
    "d2l.check_shape(C_lstm, (5, 3))\n",
    "\n",
    "print(\"Pass!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanisms\n",
    "\n",
    "Consider the following: denote by $\\mathcal{D} = \\{(\\mathbf{k}_1, \\mathbf{v}_1), \\ldots, (\\mathbf{k}_m, \\mathbf{v}_m)\\}$ a database of $m$ tuples of *keys* and *values*. Moreover, denote by $\\mathbf{q}$ a *query*. Then we can define the *attention* over $\\mathcal{D}$ as\n",
    "\n",
    "$$\n",
    "\\text{Attention}(\\mathbf{q}, \\mathcal{D}) = \\sum_{i=1}^{m} \\alpha(\\mathbf{q}, \\mathbf{k}_i) \\mathbf{v}_i\n",
    "$$\n",
    "\n",
    "where $\\alpha(\\mathbf{q}, \\mathbf{k}_i) \\in \\mathcal{R}$ $(i = 1, \\ldots, m)$ are scalar attention weights. This operation is commonly known as *attention pooling*. The term *attention* reflects the mechanismâ€™s ability to focus on specific elements in the dataset, assigning higher weights $\\alpha$ to the terms in $\\mathcal{D}$ that are deemed more relevant or significant. Consequently, the attention mechanism produces a weighted linear combination of the values in the database, emphasizing the most important components.\n",
    "\n",
    "A common strategy for ensuring that the weights sum up to 1 is to normalize them via\n",
    "\n",
    "$$\n",
    "\\alpha(\\mathbf{q}, \\mathbf{k}_i) = \\frac{\\alpha(\\mathbf{q}, \\mathbf{k}_i)}{\\sum_j \\alpha(\\mathbf{q}, \\mathbf{k}_j)}.\n",
    "$$\n",
    "\n",
    "In particular, to ensure that the weights are also nonnegative, one can resort to exponentiation. This means that we can now pick any function $a(\\mathbf{q}, \\mathbf{k})$ and then apply the softmax operation used for multinomial models to it via\n",
    "\n",
    "$$\n",
    "\\alpha(\\mathbf{q}, \\mathbf{k}_i) = \\frac{\\exp(a(\\mathbf{q}, \\mathbf{k}_i))}{\\sum_j \\exp(a(\\mathbf{q}, \\mathbf{k}_j))}.\n",
    "$$\n",
    "\n",
    "Then, we need to keep the order of magnitude of the arguments in the exponential function under control. Assume that all the elements of the query $\\mathbf{q} \\in \\mathcal{R}^d$ and the key $\\mathbf{k}_i \\in \\mathcal{R}^d$ are independent and identically drawn random variables with zero mean and unit variance. The dot product between both vectors has zero mean and a variance of $d$. To ensure that the variance of the dot product still remains 1 regardless of vector length, we use the *scaled dot product attention* scoring function. That is, we rescale the dot product by $1/\\sqrt{d}$. We thus arrive at the first commonly used attention function that is used:\n",
    "\n",
    "$$\n",
    "a(\\mathbf{q}, \\mathbf{k}_i) = \\frac{\\mathbf{q}^\\top \\mathbf{k}_i}{\\sqrt{d}}.\n",
    "$$\n",
    "\n",
    "Note that attention weights $\\alpha$ still need normalizing. We can simplify this further via the softmax operation:\n",
    "\n",
    "$$\n",
    "\\alpha(\\mathbf{q}, \\mathbf{k}_i) = \\text{softmax}(a(\\mathbf{q}, \\mathbf{k}_i)) = \\frac{\\exp(\\mathbf{q}^\\top \\mathbf{k}_i / \\sqrt{d})}{\\sum_{j=1}^m \\exp(\\mathbf{q}^\\top \\mathbf{k}_j / \\sqrt{d})}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 [code] (15 points)\n",
    "\n",
    "In sequence modeling, different input sequences often have different lengths.\n",
    "For example, consider the following batch of three sentences (each padded to the same maximum length):\n",
    "\n",
    "| Study | about | Deep    | Learning  |\n",
    "|-------|-------|---------|-----------|\n",
    "| Start | by    | code    | `<pad>` |\n",
    "| Hello | world | `<pad>` | `<pad>` |\n",
    "\n",
    "Here, the valid lengths are **[4, 3, 2]**, and `<pad>` represents padding tokens that should not affect the computation.\n",
    "\n",
    "To put them in the same batch and compute correctly, we usually apply a mask so that positions beyond the valid length are ignored â€” that is, their probabilities after softmax become zero.\n",
    "\n",
    "Implement the function ``masked_softmax``, which performs the softmax operation but masks out elements beyond the valid lengths. Then, run the sanity check cell to check your implementation.\n",
    "\n",
    "ðŸ’¡ **Hints:**\n",
    "You may find the following functions helpful (but you are not required to use them):\n",
    "- `nn.functional.softmax`\n",
    "- `torch.arange`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(X, valid_lens=None):  #@save\n",
    "    \"\"\"Perform softmax operation by masking elements on the last axis.\"\"\"\n",
    "    # Shape of X: (batch_size, num_queries, num_keys)\n",
    "    # Shape of valid_lens: (batch_size,) or (batch_size, num_queries)\n",
    "    def _sequence_mask(X, valid_lens, value=0):\n",
    "        # Shape of X: (batch_size * num_queries, num_keys)\n",
    "        # Shape of valid_lens: (batch_size * num_queries,)\n",
    "        ### YOUR CODE HERE (10 points)\n",
    "        maxlen = X.shape[1] \n",
    "        mask = torch.arange(maxlen)[None,:] < valid_lens[:,None]  # Mask: True for positions < valid_len\n",
    "        X[~mask] = value  # If False (not valid), set to value=0\n",
    "        return X\n",
    "        ### END OF YOUR CODE\n",
    "\n",
    "    if valid_lens is None:\n",
    "        ### YOUR CODE HERE (2 points)\n",
    "        return nn.functional.softmax(X,dim=-1)\n",
    "        ### END OF YOUR CODE\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_lens.dim() == 1:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "        else:\n",
    "            valid_lens = valid_lens.reshape(-1)\n",
    "        # On the last axis, replace masked elements with a very large negative\n",
    "        # value, whose exponentiation outputs 0\n",
    "        X = _sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
    "        ### YOUR CODE HERE (3 points)\n",
    "        return nn.functional.softmax(X.reshape(shape),dim=-1)\n",
    "        ### END OF YOUR CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3601, 0.3001, 0.1998, 0.1399]],\n",
       "\n",
       "        [[0.3982, 0.6018, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.2449, 0.3499, 0.4053, 0.0000]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "# Assume num_queries = 1\n",
    "masked_softmax(torch.rand(3, 1, 4), torch.tensor([4, 2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 (15 points)\n",
    "In practice, we often think of minibatches for efficiency, such as computing attention for $n$ queries and $m$ key-value pairs, where queries and keys are of dimension $d$ and values are of dimension $v$. The scaled dot product attention of queries $\\mathbf{Q} \\in \\mathcal{R}^{n \\times d}$, keys $\\mathbf{K} \\in \\mathcal{R}^{m \\times d}$, and values $\\mathbf{V} \\in \\mathcal{R}^{m \\times v}$ thus can be written as\n",
    "\n",
    "$$\n",
    "\\text{softmax} \\left( \\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{d}} \\right) \\mathbf{V} \\in \\mathcal{R}^{n \\times v}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.1 [written] (5 points)** Write the shape of queries, keys and values during the calculation of scaled dot product attention. **You should fill in the shape inside the code box**.\n",
    "\n",
    "**Question 4.2 [code] (10 points)** Implement function ``DotProductAttention`` that calculates the scaled dot product attention. Then, run the sanity check cell to check your implementation.\n",
    "\n",
    "ðŸ’¡ **Hint:**  \n",
    "- You should use the previously implemented `masked_softmax` function to handle sequences of different lengths.  \n",
    "- You may find the following functions helpful (but you are not required to use them):\n",
    "    - `torch.bmm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module):  #@save\n",
    "    \"\"\"Scaled dot product attention.\"\"\"\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        # Shape of queries: (batch_size, n, d)\n",
    "        # Shape of keys: (batch_size, m, d)\n",
    "        # Shape of values: (batch_size, m, v)\n",
    "        # Shape of valid_lens: (batch_size,) or (batch_size, num_queries)\n",
    "        ### YOUR CODE HERE\n",
    "        d = queries.shape[-1]\n",
    "        # Scaled dot product attention of queries, keys, and values\n",
    "        attention_weights = masked_softmax(torch.bmm(queries,keys.transpose(1,2)) / torch.sqrt(torch.tensor(d,dtype=torch.float32)),valid_lens)\n",
    "        ### END OF YOUR CODE\n",
    "        return torch.bmm(self.dropout(attention_weights), values), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass!\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "queries = torch.normal(0, 1, (3, 10, 4))\n",
    "keys = torch.normal(0, 1, (3, 12, 4))\n",
    "values = torch.normal(0, 1, (3, 12, 4))\n",
    "valid_lens = torch.tensor([7, 9, 12])\n",
    "\n",
    "attention = DotProductAttention(dropout=0.5)\n",
    "attention.eval()\n",
    "results, attention_weights = attention(queries, keys, values, valid_lens)\n",
    "d2l.check_shape(results, (3, 10, 4))\n",
    "print(\"Pass!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x300 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b, q, k = attention_weights.shape\n",
    "fig, axes = plt.subplots(1, b, figsize=(4*b, 3))\n",
    "if b == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    mat = attention_weights[i].detach().cpu().numpy()   # shape (num_queries, num_keys)\n",
    "    im = ax.imshow(mat, cmap='viridis', aspect='auto', interpolation='nearest')\n",
    "    ax.set_xlabel('Keys')\n",
    "    ax.set_ylabel('Queries')\n",
    "    ax.set_title(f'Batch {i+1}')\n",
    "    fig.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Seq2Seq\n",
    "Attention mechanisms can be effectively integrated into encoder-decoder architectures for sequence-to-sequence learning. Traditionally, in an RNN-based approach, all relevant information from the source sequence is encoded into a fixed-dimensional state representation by the encoder. However, rather than maintaining this stateâ€”represented by the context variable $\\mathbf{c}$ that summarizes the source sentenceâ€”as a fixed value, it can be dynamically updated. This update is based on both the original text (encoder hidden states $\\mathbf{h}_{t}$) and the previously generated text (decoder hidden states $\\mathbf{s}_{tâ€™-1}$). As a result, we obtain an updated context variable $\\mathbf{c}_{tâ€™}$ after each decoding time step $tâ€™$. This approach allows the model to adapt the context dynamically, even for input sequences of length $T$, thereby improving the ability to handle long-range dependencies and capture more nuanced information from the source sequence.\n",
    "In this case, the context variable is the output of attention pooling:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{c}_{t'} = \\sum_{t=1}^{T} \\alpha(\\mathbf{s}_{t'-1}, \\mathbf{h}_t) \\mathbf{h}_t.\n",
    "\\end{equation}\n",
    "\n",
    "We used $\\mathbf{s}_{t'-1}$ as the query, and $\\mathbf{h}_t$ as both the key and the value. Note that $\\mathbf{c}_{t'}$ is then used to generate the state $\\mathbf{s}_{t'}$ and to generate a new token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 [code] (10 points)\n",
    "Implement the RNN decoder in the ``Seq2SeqAttentionDecoder`` class. The decoderâ€™s state is initialized using three components: \n",
    "\n",
    "(i) the hidden states of the encoderâ€™s last layer across all time steps, which are utilized as keys and values for the attention mechanism; \n",
    "\n",
    "(ii) the hidden state of the encoderâ€™s final time step at all layers, which initializes the decoderâ€™s hidden state; and \n",
    "\n",
    "(iii) the valid length of the encoder to exclude padding tokens during attention pooling. \n",
    "\n",
    "During each decoding time step, the hidden state of the decoderâ€™s final layer from the previous step is used as the query for the attention mechanism. The attention mechanismâ€™s output is then concatenated with the input embedding to form the input for the RNN decoder, effectively guiding the generation process with context from both the source sequence and previous decoder outputs.\n",
    "\n",
    "Then, run the sanity check cell to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqAttentionDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0):\n",
    "        super().__init__()\n",
    "        self.attention = d2l.AdditiveAttention(num_hiddens, dropout)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(\n",
    "            embed_size + num_hiddens, num_hiddens, num_layers,\n",
    "            dropout=dropout)\n",
    "        self.dense = nn.LazyLinear(vocab_size)\n",
    "        self.apply(d2l.init_seq2seq)\n",
    "\n",
    "    def init_state(self, enc_outputs, enc_valid_lens):\n",
    "        # Shape of outputs: (num_steps_enc, batch_size, num_hiddens).\n",
    "        # Shape of hidden_state: (num_layers, batch_size, num_hiddens)\n",
    "        outputs, hidden_state = enc_outputs\n",
    "        return (outputs.permute(1, 0, 2), hidden_state, enc_valid_lens)\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        # Shape of enc_outputs: (batch_size, num_steps_enc, num_hiddens)\n",
    "        # Shape of hidden_state: (num_layers, batch_size, num_hiddens)\n",
    "        enc_outputs, hidden_state, enc_valid_lens = state\n",
    "        # Shape of the output X: (num_steps_dec, batch_size, embed_size)\n",
    "        X = self.embedding(X).permute(1, 0, 2)\n",
    "        outputs = []\n",
    "        ### YOUR CODE HERE \n",
    "        for x in X: \n",
    "            # Hidden state of the decoder's final layer is used as the query\n",
    "            query = hidden_state[-1].unsqueeze(1)  # Add a dimension to the hidden state so shape becomes (batch_size, 1, num_hiddens)\n",
    "            # Attention step, computes a weighted combination of encoder outputs according to how similar query is to each key (enc_output)\n",
    "            context = self.attention(query, enc_outputs, enc_outputs, enc_valid_lens) \n",
    "            # Attention mechanism's output is concat with the input embedding to form the input for rnn decoder\n",
    "            rnn_input = torch.cat((context, x.unsqueeze(1)), dim=-1)\n",
    "            # Feed this combined information into the rnn, decoder updates its internal memory\n",
    "            out, hidden_state = self.rnn(rnn_input.permute(1, 0, 2), hidden_state)\n",
    "            outputs.append(out) # store the per-timestep output (word generated)\n",
    "        # After finishing all steps, concat all the per-timestep outputs\n",
    "        # Convert the decoder's internal representation into actual word predictions\n",
    "        outputs = self.dense(torch.cat(outputs, dim=0))  # Apply a dense layer\n",
    "        ### END OF YOUR CODE\n",
    "        return outputs.permute(1, 0, 2), [enc_outputs, hidden_state,\n",
    "                                          enc_valid_lens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass!\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "vocab_size, embed_size, num_hiddens, num_layers = 10, 8, 16, 2\n",
    "batch_size = 4\n",
    "num_steps_enc = 5\n",
    "num_steps_dec = 7\n",
    "\n",
    "# instantiate encoder and decoder\n",
    "encoder = d2l.Seq2SeqEncoder(vocab_size, embed_size, num_hiddens, num_layers)\n",
    "decoder = Seq2SeqAttentionDecoder(vocab_size, embed_size, num_hiddens, num_layers)\n",
    "\n",
    "# dummy inputs\n",
    "X_enc = torch.zeros((batch_size, num_steps_enc), dtype=torch.long)\n",
    "X_dec = torch.zeros((batch_size, num_steps_dec), dtype=torch.long)\n",
    "\n",
    "# initialize decoder state using encoder outputs\n",
    "state = decoder.init_state(encoder(X_enc), None)\n",
    "\n",
    "# forward pass through decoder\n",
    "output, state = decoder(X_dec, state)\n",
    "\n",
    "d2l.check_shape(output, (batch_size, num_steps_dec, vocab_size))\n",
    "d2l.check_shape(state[0], (batch_size, num_steps_enc, num_hiddens))  # encoder outputs\n",
    "d2l.check_shape(state[1], (num_layers, batch_size, num_hiddens)) # hidden_state\n",
    "print(\"Pass!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multihead Attention\n",
    "Rather than relying on a single attention pooling operation, the queries, keys, and values can be transformed through $h$ independently learned linear projections. These $h$ projected queries, keys, and values are then processed in parallel through attention pooling. Afterward, the $h$ resulting attention outputs, known as ``heads``, are concatenated and passed through another learned linear projection to generate the final output. This architecture, referred to as ``multi-head attention``, allows each attention head to focus on different parts of the input, enabling the model to capture a wider range of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ma](Multihead_attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a query $\\mathbf{q} \\in \\mathcal{R}^{d_q}$, a key $\\mathbf{k} \\in \\mathcal{R}^{d_k}$, and a value $\\mathbf{v} \\in \\mathcal{R}^{d_v}$, each attention head $\\mathbf{h}_i$ $(i = 1, \\ldots, h)$ is computed as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{h}_i = f(\\mathbf{W}_i^{(q)} \\mathbf{q}, \\mathbf{W}_i^{(k)} \\mathbf{k}, \\mathbf{W}_i^{(v)} \\mathbf{v}) \\in \\mathcal{R}^{p_v},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{W}_i^{(q)} \\in \\mathcal{R}^{p_q \\times d_q}$, $\\mathbf{W}_i^{(k)} \\in \\mathcal{R}^{p_k \\times d_k}$, and $\\mathbf{W}_i^{(v)} \\in \\mathcal{R}^{p_v \\times d_v}$ are learnable parameters and $f$ is attention pooling. The multi-head attention output is another linear transformation via learnable parameters $\\mathbf{W}_o \\in \\mathcal{R}^{p_o \\times hp_v}$ of the concatenation of $h$ heads:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{W}_o \n",
    "\\begin{bmatrix}\n",
    "\\mathbf{h}_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{h}_h\n",
    "\\end{bmatrix}\n",
    "\\in \\mathcal{R}^{p_o}. \n",
    "\\end{equation}\n",
    "\n",
    "Based on this design, each head may attend to different parts of the input. More sophisticated functions than the simple weighted average can be expressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 [written] (10 points)\n",
    "Please describe the benefits of using multi-head attention instead of single head attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since a wider range of information can be captured, more diverse and complex relationships can be learned, providing a more comprehensive understanding of the data.\n",
    "\n",
    "- It would also increase the model's capacity to learn and represent complex data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7 [code] (30 points)\n",
    "In this implementation, we choose the scaled dot product attention for each head of the multi-head attention. To avoid significant growth of computational cost and parameterization cost, we set $p_q = p_k = p_v = p_o / h$. Note that $h$ heads can be computed in parallel if we set the number of outputs of linear transformations for the query, key, and value to $p_qh = p_kh = p_vh = p_o$. In the following implementation, $p_o$ is specified via the argument ``num_hiddens``.\n",
    "\n",
    "\n",
    "To allow for parallel computation of multiple heads, the ``MultiHeadAttention`` class uses two transposition methods ``transpose_qkv`` and ``transpose_output``. Specifically, the ``transpose_output`` method reverses the operation of the ``transpose_qkv`` method.\n",
    "\n",
    "**Question 7.1 [code] (10 points)** Implement function ``transpose_qkv``, which is the transposition for parallel computation of multiple attention heads.\n",
    "\n",
    "**Question 7.2 [code] (10 points)** Implement function ``transpose_output`` that reverse the operation of ``transpose_qkv``.\n",
    "\n",
    "**Question 7.3 [code] (10 points)**\n",
    "Complete `MultiHeadAttention` class. (Hint: you can use the two function you defined in question 7.1 and 7.2 .)\n",
    "\n",
    "\n",
    "ðŸ’¡ **Hints:**\n",
    "You may find the following functions helpful (but you are not required to use them):\n",
    "- `torch.repeat_interleave`\n",
    "- `torch.reshape`\n",
    "- `torch.permute`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):  #@save\n",
    "    \"\"\"Multi-head attention.\"\"\"\n",
    "    def __init__(self, num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = d2l.DotProductAttention(dropout)\n",
    "        self.W_q = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "        self.W_k = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "        self.W_v = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "        self.W_o = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        # Shape of queries, keys, or values:\n",
    "        # (batch_size, no. of queries or key-value pairs, num_hiddens)\n",
    "        # Shape of valid_lens: (batch_size,) or (batch_size, num_queries)\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        # Project queries, keys and values through linear layers and transpose\n",
    "        queries = self.transpose_qkv(self.W_q(queries))\n",
    "        keys = self.transpose_qkv(self.W_k(keys))\n",
    "        values = self.transpose_qkv(self.W_v(values))\n",
    "        # Adjust valid lens for multiple heads\n",
    "        if valid_lens is not None:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, repeats=self.num_heads, dim=0)\n",
    "        # Process in parallel through attention pooling\n",
    "        output = self.attention(queries, keys, values, valid_lens)\n",
    "        # Resulting heads are concatenated and passed through another learned linear projection to generate the final output\n",
    "        output_concat = self.transpose_output(output)\n",
    "        ### END OF YOUR CODE\n",
    "        return self.W_o(output_concat)\n",
    "    \n",
    "    def transpose_qkv(self, X):\n",
    "        \"\"\"Transposition for parallel computation of multiple attention heads.\"\"\"\n",
    "        # Shape of X: (batch_size, no. of queries or key-value pairs, num_hiddens)\n",
    "\n",
    "        ### YOUR CODE HERE\n",
    "        batch_size, num_queries, num_hiddens = X.shape\n",
    "        num_heads=self.num_heads\n",
    "        X = torch.reshape(X,(batch_size*num_heads, num_queries, num_hiddens//num_heads))\n",
    "        return X\n",
    "        # Shape of output: (batch_size*num_heads, no. of queries or key-value pairs, num_hiddens / num_heads)\n",
    "        ### END OF YOUR CODE\n",
    "\n",
    "    def transpose_output(self, X):\n",
    "        \"\"\"Reverse the operation of transpose_qkv.\"\"\"\n",
    "        # Shape of X: (batch_size * num_heads, num_queries, num_hiddens / num_heads)\n",
    "        ### YOUR CODE HERE\n",
    "        batchsize_times_numheads, seq_len, numhiddens_div_numheads = X.shape\n",
    "        num_heads=self.num_heads\n",
    "        batch_size = batchsize_times_numheads // num_heads\n",
    "        num_hiddens = numhiddens_div_numheads * num_heads\n",
    "        X = torch.reshape(X,(batch_size, seq_len,num_hiddens))\n",
    "        return X\n",
    "        # Shape of output: (batch_size, seq_len, num_hiddens)\n",
    "        ### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass!\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "num_hiddens, num_heads = 100, 5\n",
    "attention = MultiHeadAttention(num_hiddens, num_heads, 0.5)\n",
    "batch_size, num_queries, num_kvpairs = 2, 4, 6\n",
    "valid_lens = torch.tensor([3, 6])\n",
    "X = torch.ones((batch_size, num_queries, num_hiddens))\n",
    "Y = torch.ones((batch_size, num_kvpairs, num_hiddens))\n",
    "d2l.check_shape(attention(X, Y, Y, valid_lens),\n",
    "                (batch_size, num_queries, num_hiddens))\n",
    "print(\"Pass!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
